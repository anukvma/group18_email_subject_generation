# Email Subject Generation (Group 18)

## Description
Generate a succinct subject line from the body of an email.
Email Subject Line Generation task involves identifying the most important sentences in an email and abstracting their message into just a few words. The project provides an opportunity to work with generative models in NLP, specifically using GPT-2 variants, and to explore different metrics for evaluating text generation.

## DataSet
Dataset used is from the below repository for fine tuning the models
The Annotated Enron Subject Line Corpus: https://github.com/ryanzhumich/AESLC

## Models
The following models are fine tuned 
| LLM     	| Framework             | Model Type        | Training Steps       	| Evaluation Method    	| 
|---------	|---------------------	|-------------------|---------------------	|----------------------	|
| Mistral 	| unsloth             	| 4 bit quantized 	| 60 	                  | ROUGE Score         	|
| Llama3  	| unsloth             	| 4 bit quantized 	| 60  	                | ROUGE Score           |
| T5      	| HuggingFace           | Base model       	| 200                  	| ROUGE Score          	|
| Bart    	| HuggingFace           | Base model       	| 200                  	| ROUGE Score          	|

## Training Steps

### Mistral

### LLAMA3

### T5

### Bart

## Result
| LLM     	| Rogue1              	| Rogue2               	| RougeL              	| RogueLSum            	|
|---------	|---------------------	|----------------------	|---------------------	|----------------------	|
| Mistral 	| 0.04175057546404236 	| 0.015307029349338995 	| 0.03865576026979294 	| 0.040112317820734385 	|
| Llama3  	| 0.044540652323630435 	| 0.016282087086038018 	| 0.03984053234184394  	| 0.04157418257161926  	|
| T5      	| 0.144567            	| 0.070306             	| 0.140258            	| 0.141119             	|
| Bart    	| 0.267373            	| 0.134597             	| 0.249993            	| 0.250012             	|

## Observations
1. Generative models are very large to be trained on base model, we had to use quantized versions.
2. Email subjects generated by both Generative and Seq2Seq Model is contextually correct.
3. Generative model use work synonyms, hence Rogue score is very low.
4. Seq2Seq models pick up the words from Email content, which is expected in this experiment, hence Rouge score is high.
5. Bart and T5 both models are encoder-decoder type models, but Bart performs better than T5 due to:
  1. Type of corpus these models are trained with is different
  2. T5 randomly drop 15% tokens during training, but Bart is trained by corrupting documents and then optimizing a reconstruction loss
  3. T5 uses relative positional encoding, where Bart uses absolute positional encoding
  4. Bart initializes parameters from N (0, 0.02), where T5 N(0, 1/sqrt(d_model))
     
## HuggingFace Demo URL
https://huggingface.co/spaces/GSridhar1982/EmailSubjectGenerationDemo
